{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ea7a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/yl2672496l/Yue/code/jupyter_env/lib/python3.12/site-packages (0.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: Failed to prepare distributions\n",
      "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: Failed to download and build `numpy==1.22.4`\n",
      "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: Build backend failed to determine requirements with `\u001b[32mbuild_wheel()\u001b[39m` (exit status: 1)\n",
      "\n",
      "\u001b[31m[stderr]\u001b[39m\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 8, in <module>\n",
      "  File \"/home/yl2672496l/.cache/uv/builds-v0/.tmpTrHZYU/lib/python3.12/site-packages/setuptools/__init__.py\", line 10, in <module>\n",
      "    import distutils.core\n",
      "ModuleNotFoundError: No module named 'distutils'\n",
      "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: distutils was removed from the standard library in Python 3.12. Consider adding a constraint (like `numpy >1.22.4`) to avoid building a version of numpy that depends on distutils.\n"
     ]
    }
   ],
   "source": [
    "!pip install uv\n",
    "!uv pip install -q autogluon.timeseries --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1558947",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39636538",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('GD030A_S.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recover_timestamp function\n",
    "def recover_timestamp(data):\n",
    "    # Combine 'date' and 'time' to form a datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'].astype(str) + ':00', format='%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Set 'datetime' as index\n",
    "    data = data.set_index('datetime')\n",
    "\n",
    "    # Create a complete range of timestamps with hourly frequency\n",
    "    full_time_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='H')\n",
    "\n",
    "    # Reindex the data to include all timestamps, filling missing rows with NaN\n",
    "    data_full = data.reindex(full_time_range)\n",
    "\n",
    "    return data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_full = recover_timestamp(my_data)\n",
    "traffic_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb29c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = traffic_full[:'2022-02-28 23:00:00']\n",
    "valid_set = traffic_full['2022-03-01 00:00:00':'2022-12-31 23:00:00']\n",
    "test_set = traffic_full['2023-01-01 00:00:00':]\n",
    "print('Proportion of train_set : {:.4f}'.format(len(train_set)/len(traffic_full)))\n",
    "print('Proportion of valid_set : {:.4f}'.format(len(valid_set)/len(traffic_full)))\n",
    "print('Proportion of test_set : {:.4f}'.format(len(test_set)/len(traffic_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, input_length, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Creates sequences for time series data, excluding any sequences containing NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the data. Must include the 'flow' column.\n",
    "    - input_length: int, number of past time steps to include in each input sequence.\n",
    "    - forecast_horizon: int, number of future steps to predict.\n",
    "\n",
    "    Returns:\n",
    "    - sequences_df: pandas DataFrame containing all sequences with input lags and output leads,\n",
    "                    and an 'item_id' column to separate each sequence, including timestamps.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    item_id = 0  # To assign unique ID to each sequence\n",
    "\n",
    "    for i in range(input_length, len(data) - forecast_horizon + 1):\n",
    "        # Extract the input sequence\n",
    "        X_seq = data.iloc[i - input_length:i]['flow'].values\n",
    "        X_timestamps = data.iloc[i - input_length:i].index\n",
    "        # Extract the target sequence\n",
    "        y_seq = data.iloc[i:i + forecast_horizon]['flow'].values\n",
    "        y_timestamps = data.iloc[i:i + forecast_horizon].index\n",
    "\n",
    "        # Check for NaN values in the input sequence and target sequence\n",
    "        if not np.isnan(X_seq).any() and not np.isnan(y_seq).any():\n",
    "            # Convert X_seq and y_seq to DataFrames, including their timestamps\n",
    "            x_df = pd.DataFrame({'target': X_seq, 'timestamp': X_timestamps})\n",
    "            y_df = pd.DataFrame({'target': y_seq, 'timestamp': y_timestamps})\n",
    "\n",
    "            # Concatenate X_df and y_df\n",
    "            seq_df = pd.concat([x_df, y_df], ignore_index=True)\n",
    "\n",
    "            # Add 'item_id' column\n",
    "            seq_df['item_id'] = str(item_id)\n",
    "            item_id += 1  # Increment item_id\n",
    "\n",
    "            # Append to list\n",
    "            sequences.append(seq_df)\n",
    "        else:\n",
    "            # Optionally, log or count the skipped sequences\n",
    "            pass  # Simply skip sequences with NaNs\n",
    "\n",
    "    # Concatenate all sequences into one DataFrame\n",
    "    sequences_df = pd.concat(sequences)\n",
    "    sequences_df['item_id'] = sequences_df['item_id'].astype('str')\n",
    "    sequences_df['target'] = sequences_df['target'].astype('float32')\n",
    "\n",
    "    return sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [24 * i for i in range(1, 22)]\n",
    "prediction_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160dc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data_dict = defaultdict(dict)\n",
    "\n",
    "for length in input_lengths:\n",
    "    print(f\"Processing input length: {length}\")\n",
    "\n",
    "    # Create sequences with forecast_horizon=6\n",
    "    test = create_sequences(test_set, length, forecast_horizon=6)\n",
    "\n",
    "    # Store in the dictionary\n",
    "    data_dict[length]['test'] = test\n",
    "\n",
    "    # Print shapes and ensure no NaNs\n",
    "    print(f\"  test shape: {test.index.max()+1}, {len(test.item_id.unique())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9fc1e",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee23b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronos_zero_shot(input_lengths, prediction_length, model_type):\n",
    "    results_dict = defaultdict(dict)\n",
    "\n",
    "    for length in input_lengths:\n",
    "        print(f\"Processing input length: {length}\")\n",
    "\n",
    "        # prepare the data\n",
    "        test_data = TimeSeriesDataFrame(data_dict[length]['test'])\n",
    "        test_data_ready, test_data = test_data.train_test_split(prediction_length)\n",
    "\n",
    "        predictor = TimeSeriesPredictor(prediction_length=prediction_length).fit(test_data_ready, presets=model_type)\n",
    "        predictions = predictor.predict(test_data_ready)\n",
    "\n",
    "        ground_truth_df = test_data.groupby(level='item_id').tail(6)\n",
    "        predicted_df = predictions[['mean']]\n",
    "\n",
    "        results_dict[length]['Predicted_Results'] = predicted_df\n",
    "        results_dict[length]['Gound_Truth'] = ground_truth_df\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb71991",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = chronos_zero_shot(input_lengths, prediction_length, \"chronos_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metrics(df_true, df_pred):\n",
    "    df_merged = pd.merge(df_true, df_pred, on=['item_id', 'timestamp'], how='inner')\n",
    "\n",
    "    # Step 3: Assign prediction step numbers (1 to 6) within each 'item_id'\n",
    "    df_merged['step'] = df_merged.groupby('item_id').cumcount() + 1\n",
    "\n",
    "    # Step 4: Calculate error metrics\n",
    "    df_merged['error'] = df_merged['mean'] - df_merged['target']\n",
    "    df_merged['abs_error'] = df_merged['error'].abs()\n",
    "    df_merged['squared_error'] = df_merged['error'] ** 2\n",
    "\n",
    "    # Handle division by zero in MAPE calculation by adding a very small number (epsilon)\n",
    "    epsilon = 1e-10\n",
    "    df_merged['abs_percentage_error'] = df_merged['abs_error'] / (df_merged['target'].abs() + epsilon)\n",
    "\n",
    "    # Step 5: Calculate overall metrics\n",
    "    overall_mse = df_merged['squared_error'].mean()\n",
    "    overall_rmse = np.sqrt(overall_mse)\n",
    "    overall_mae = df_merged['abs_error'].mean()\n",
    "    overall_mape = df_merged['abs_percentage_error'].mean() * 100\n",
    "\n",
    "    print(\"Overall Evaluation Metrics:\")\n",
    "    print(f\"MSE: {overall_mse:.4f}\")\n",
    "    print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"MAE: {overall_mae:.4f}\")\n",
    "    print(f\"MAPE: {overall_mape:.2f}%\")\n",
    "\n",
    "    # Step 6: Calculate metrics for each prediction step\n",
    "    metrics_by_step = df_merged.groupby('step').agg({\n",
    "      'squared_error': 'mean',\n",
    "      'abs_error': 'mean',\n",
    "      'abs_percentage_error': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    metrics_by_step['MSE'] = metrics_by_step['squared_error']\n",
    "    metrics_by_step['RMSE'] = np.sqrt(metrics_by_step['squared_error'])\n",
    "    metrics_by_step['MAE'] = metrics_by_step['abs_error']\n",
    "    metrics_by_step['MAPE'] = metrics_by_step['abs_percentage_error'] * 100\n",
    "\n",
    "    # Select relevant columns and round for better readability\n",
    "    metrics_by_step = metrics_by_step[['step', 'MSE', 'RMSE', 'MAE', 'MAPE']]\n",
    "\n",
    "    return metrics_by_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for length in input_lengths:\n",
    "    print(f\"Processing input length: {length}\")\n",
    "\n",
    "    predict = results_dict[length]['Predicted_Results']\n",
    "    truth = results_dict[length]['Gound_Truth']\n",
    "    evaluation_df = cal_metrics(predict, truth)\n",
    "    display(evaluation_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
