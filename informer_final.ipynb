{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f7f33-1987-4932-9803-883ede914b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
    "#!git clone https://github.com/zhouhaoyi/ETDataset.git\n",
    "# modify the code based on my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4301a-419a-40b0-98e8-eea762441e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install scikit_learn\n",
    "# !{sys.executable} -m pip install torch\n",
    "# !{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6cc0a2-7f97-4bc2-b439-c7b775a9cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a07922-9f8e-44bb-825a-b576a86a90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']\n",
    "\n",
    "from data.data_loader import Dataset_Custom\n",
    "from utils.metrics import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd148590-d5a3-4a2b-afa4-77b77d84a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6373961a-d2b9-4fbb-96a5-9ad2c3fbdc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    gpu_name = torch.cuda.get_device_name(i)\n",
    "    print(f\"GPU {i}: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1c08a-8be5-4e5c-a69c-b838fc3803d5",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5eb9c8-e5e9-43e4-b287-564297d0b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('../GD030A_S.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02c837f-f057-48fc-ad77-aa5d7c99d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recover_timestamp function\n",
    "def recover_timestamp(data):\n",
    "    # Combine 'date' and 'time' to form a datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'].astype(str) + ':00', format='%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Set 'datetime' as index\n",
    "    data = data.set_index('datetime')\n",
    "\n",
    "    # Create a complete range of timestamps with hourly frequency\n",
    "    full_time_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='H')\n",
    "\n",
    "    # Reindex the data to include all timestamps, filling missing rows with NaN\n",
    "    data_full = data.reindex(full_time_range)\n",
    "\n",
    "    return data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb2e9e-0912-4c8a-8ccd-bca7f1fc91d9",
   "metadata": {},
   "source": [
    "Custom data (xxx.csv) has to include at least 2 features: date(format: YYYY-MM-DD hh:mm:ss) and target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa5669-e577-40ea-9c59-6e998abacda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_full = recover_timestamp(my_data)\n",
    "traffic_full.drop(['date', 'time'], axis=1, inplace=True)\n",
    "traffic_full = traffic_full.reset_index(names='date')\n",
    "traffic_full['date'] = traffic_full['date'].astype(str)\n",
    "traffic_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e64353-3a50-4922-b0ae-81f1f592c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traffic_full.to_csv('traffic_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04616932-2190-4b35-bceb-1bf11fa3e582",
   "metadata": {},
   "source": [
    "## Run training directly using bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6890335e-1970-46ea-a2f0-9f3a9fb4938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the shell commands below using the current Python environment\n",
    "os.environ[\"PATH\"] = f\"{sys.prefix}/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf9133b-3e63-4228-8340-98bc089bf852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yl2672496l/Yue/code/transformer/Informer2020\n"
     ]
    }
   ],
   "source": [
    "cd Informer2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce82bd-438d-4fec-b166-87b2984ae0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "!python -u main_informer.py --model informer --data traffic_full --root_path ../ --features S --seq_len 24 --label_len 24 --pred_len 6 --e_layers 2 --d_layers 1 --attn prob --des 'Exp' --itr 1 #--inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297f6900-cd2b-4569-8ef9-4b2e4dd2b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 48, 72, 96, 120, 144, 168, 192, 216, 240, 264, 288, 312, 336, 360, 384, 408, 432, 456, 480, 504]\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [24 * i for i in range(1, 22)]\n",
    "print(seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97623185-1858-4ecd-99d3-bb9f6ff19c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seq_len=24\n",
      "Args in experiment:\n",
      "Namespace(model='informer', data='traffic_full', root_path='../', data_path='traffic_full.csv', features='S', target='flow', freq='h', checkpoints='./checkpoints/', seq_len=24, label_len=24, pred_len=6, enc_in=1, dec_in=1, c_out=1, d_model=512, n_heads=8, e_layers=2, d_layers=1, s_layers=[3, 2, 1], d_ff=2048, factor=5, padding=0, distil=True, dropout=0.05, attn='prob', embed='timeF', activation='gelu', output_attention=False, do_predict=False, mix=True, cols=None, num_workers=0, itr=10, train_epochs=6, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, inverse=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', detail_freq='h')\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_traffic_full_ftS_sl24_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 24515\n",
      "[DEBUG] Original dataset length: 24515\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 23049\n",
      "val 3503\n",
      "[DEBUG] Original dataset length: 3503\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 3471\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6357\n",
      "\titers: 100, epoch: 1 | loss: 0.5307340\n",
      "\tspeed: 0.0371s/iter; left time: 156.6516s\n",
      "\titers: 200, epoch: 1 | loss: 0.4023372\n",
      "\tspeed: 0.0227s/iter; left time: 93.3734s\n",
      "\titers: 300, epoch: 1 | loss: 0.3258615\n",
      "\tspeed: 0.0253s/iter; left time: 101.5862s\n",
      "\titers: 400, epoch: 1 | loss: 0.2069519\n",
      "\tspeed: 0.0249s/iter; left time: 97.4393s\n",
      "\titers: 500, epoch: 1 | loss: 0.2388484\n",
      "\tspeed: 0.0226s/iter; left time: 86.4546s\n",
      "\titers: 600, epoch: 1 | loss: 0.1579532\n",
      "\tspeed: 0.0221s/iter; left time: 82.1602s\n",
      "\titers: 700, epoch: 1 | loss: 0.1898353\n",
      "\tspeed: 0.0231s/iter; left time: 83.7786s\n",
      "Epoch: 1 cost time: 17.380558252334595\n",
      "Epoch: 1, Steps: 720 | Train Loss: 0.2673535 Vali Loss: 0.3295307 Test Loss: 0.2546920\n",
      "Validation loss decreased (inf --> 0.329531).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1886444\n",
      "\tspeed: 0.0557s/iter; left time: 195.0732s\n",
      "\titers: 200, epoch: 2 | loss: 0.1523376\n",
      "\tspeed: 0.0250s/iter; left time: 85.1888s\n",
      "\titers: 300, epoch: 2 | loss: 0.2702779\n",
      "\tspeed: 0.0232s/iter; left time: 76.7439s\n",
      "\titers: 400, epoch: 2 | loss: 0.2101504\n",
      "\tspeed: 0.0227s/iter; left time: 72.5069s\n",
      "\titers: 500, epoch: 2 | loss: 0.2500958\n",
      "\tspeed: 0.0238s/iter; left time: 73.7768s\n",
      "\titers: 600, epoch: 2 | loss: 0.1775596\n",
      "\tspeed: 0.0231s/iter; left time: 69.3010s\n",
      "\titers: 700, epoch: 2 | loss: 0.2105108\n",
      "\tspeed: 0.0239s/iter; left time: 69.3244s\n",
      "Epoch: 2 cost time: 17.15953230857849\n",
      "Epoch: 2, Steps: 720 | Train Loss: 0.2147536 Vali Loss: 0.3164872 Test Loss: 0.2462002\n",
      "Validation loss decreased (0.329531 --> 0.316487).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2595573\n",
      "\tspeed: 0.0577s/iter; left time: 160.3398s\n",
      "\titers: 200, epoch: 3 | loss: 0.1365854\n",
      "\tspeed: 0.0228s/iter; left time: 61.0125s\n",
      "\titers: 300, epoch: 3 | loss: 0.1282246\n",
      "\tspeed: 0.0237s/iter; left time: 61.2922s\n",
      "\titers: 400, epoch: 3 | loss: 0.1502061\n",
      "\tspeed: 0.0244s/iter; left time: 60.5839s\n",
      "\titers: 500, epoch: 3 | loss: 0.1627884\n",
      "\tspeed: 0.0247s/iter; left time: 58.7921s\n",
      "\titers: 600, epoch: 3 | loss: 0.1417372\n",
      "\tspeed: 0.0261s/iter; left time: 59.5126s\n",
      "\titers: 700, epoch: 3 | loss: 0.2281683\n",
      "\tspeed: 0.0241s/iter; left time: 52.5882s\n",
      "Epoch: 3 cost time: 17.39469265937805\n",
      "Epoch: 3, Steps: 720 | Train Loss: 0.1858803 Vali Loss: 0.3305480 Test Loss: 0.2440670\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1435963\n",
      "\tspeed: 0.0549s/iter; left time: 113.1053s\n",
      "\titers: 200, epoch: 4 | loss: 0.2117333\n",
      "\tspeed: 0.0229s/iter; left time: 44.8450s\n",
      "\titers: 300, epoch: 4 | loss: 0.2248652\n",
      "\tspeed: 0.0239s/iter; left time: 44.4296s\n",
      "\titers: 400, epoch: 4 | loss: 0.1547351\n",
      "\tspeed: 0.0262s/iter; left time: 46.0533s\n",
      "\titers: 500, epoch: 4 | loss: 0.1519158\n",
      "\tspeed: 0.0229s/iter; left time: 38.0116s\n",
      "\titers: 600, epoch: 4 | loss: 0.2454980\n",
      "\tspeed: 0.0235s/iter; left time: 36.6820s\n",
      "\titers: 700, epoch: 4 | loss: 0.1273056\n",
      "\tspeed: 0.0234s/iter; left time: 34.2141s\n",
      "Epoch: 4 cost time: 17.053305864334106\n",
      "Epoch: 4, Steps: 720 | Train Loss: 0.1737216 Vali Loss: 0.3094113 Test Loss: 0.2385577\n",
      "Validation loss decreased (0.316487 --> 0.309411).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1051327\n",
      "\tspeed: 0.0555s/iter; left time: 74.4565s\n",
      "\titers: 200, epoch: 5 | loss: 0.1565659\n",
      "\tspeed: 0.0264s/iter; left time: 32.7879s\n",
      "\titers: 300, epoch: 5 | loss: 0.1633752\n",
      "\tspeed: 0.0226s/iter; left time: 25.7715s\n",
      "\titers: 400, epoch: 5 | loss: 0.1201267\n",
      "\tspeed: 0.0242s/iter; left time: 25.1685s\n",
      "\titers: 500, epoch: 5 | loss: 0.3585312\n",
      "\tspeed: 0.0224s/iter; left time: 21.1215s\n",
      "\titers: 600, epoch: 5 | loss: 0.1252295\n",
      "\tspeed: 0.0237s/iter; left time: 19.8921s\n",
      "\titers: 700, epoch: 5 | loss: 0.2051162\n",
      "\tspeed: 0.0254s/iter; left time: 18.8137s\n",
      "Epoch: 5 cost time: 17.379180192947388\n",
      "Epoch: 5, Steps: 720 | Train Loss: 0.1652112 Vali Loss: 0.3172761 Test Loss: 0.2406321\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1499949\n",
      "\tspeed: 0.0587s/iter; left time: 36.4521s\n",
      "\titers: 200, epoch: 6 | loss: 0.1414848\n",
      "\tspeed: 0.0237s/iter; left time: 12.3275s\n",
      "\titers: 300, epoch: 6 | loss: 0.1727799\n",
      "\tspeed: 0.0225s/iter; left time: 9.4829s\n",
      "\titers: 400, epoch: 6 | loss: 0.1093690\n",
      "\tspeed: 0.0232s/iter; left time: 7.4437s\n",
      "\titers: 500, epoch: 6 | loss: 0.1825032\n",
      "\tspeed: 0.0230s/iter; left time: 5.0748s\n",
      "\titers: 600, epoch: 6 | loss: 0.1198457\n",
      "\tspeed: 0.0254s/iter; left time: 3.0741s\n",
      "\titers: 700, epoch: 6 | loss: 0.2176303\n",
      "\tspeed: 0.0225s/iter; left time: 0.4725s\n",
      "Epoch: 6 cost time: 16.84920048713684\n",
      "Epoch: 6, Steps: 720 | Train Loss: 0.1605771 Vali Loss: 0.3086809 Test Loss: 0.2407491\n",
      "Validation loss decreased (0.309411 --> 0.308681).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      ">>>>>>>testing : informer_traffic_full_ftS_sl24_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6357\n",
      "Test cost time: 1.6510s\n",
      "test shape: (198, 32, 6, 1) (198, 32, 6, 1)\n",
      "test shape: (6336, 6, 1) (6336, 6, 1)\n",
      "mse:0.24100807309150696, mae:0.32721924781799316\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_traffic_full_ftS_sl24_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 24515\n",
      "[DEBUG] Original dataset length: 24515\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 23049\n",
      "val 3503\n",
      "[DEBUG] Original dataset length: 3503\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 3471\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6357\n",
      "\titers: 100, epoch: 1 | loss: 0.3297082\n",
      "\tspeed: 0.0225s/iter; left time: 95.0146s\n",
      "\titers: 200, epoch: 1 | loss: 0.3232750\n",
      "\tspeed: 0.0233s/iter; left time: 96.1123s\n",
      "\titers: 300, epoch: 1 | loss: 0.2319655\n",
      "\tspeed: 0.0268s/iter; left time: 107.5726s\n",
      "\titers: 400, epoch: 1 | loss: 0.2334506\n",
      "\tspeed: 0.0245s/iter; left time: 96.2463s\n",
      "\titers: 500, epoch: 1 | loss: 0.1415859\n",
      "\tspeed: 0.0247s/iter; left time: 94.4250s\n",
      "\titers: 600, epoch: 1 | loss: 0.2453099\n",
      "\tspeed: 0.0248s/iter; left time: 92.1808s\n",
      "\titers: 700, epoch: 1 | loss: 0.3617607\n",
      "\tspeed: 0.0231s/iter; left time: 83.5987s\n",
      "Epoch: 1 cost time: 17.449249982833862\n",
      "Epoch: 1, Steps: 720 | Train Loss: 0.2702333 Vali Loss: 0.3357077 Test Loss: 0.2555166\n",
      "Validation loss decreased (inf --> 0.335708).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2761350\n",
      "\tspeed: 0.0584s/iter; left time: 204.4965s\n",
      "\titers: 200, epoch: 2 | loss: 0.2655640\n",
      "\tspeed: 0.0239s/iter; left time: 81.1805s\n",
      "\titers: 300, epoch: 2 | loss: 0.0969127\n",
      "\tspeed: 0.0242s/iter; left time: 80.0005s\n",
      "\titers: 400, epoch: 2 | loss: 0.1444433\n",
      "\tspeed: 0.0230s/iter; left time: 73.6093s\n",
      "\titers: 500, epoch: 2 | loss: 0.2960366\n",
      "\tspeed: 0.0230s/iter; left time: 71.1745s\n",
      "\titers: 600, epoch: 2 | loss: 0.3477420\n",
      "\tspeed: 0.0224s/iter; left time: 67.2491s\n",
      "\titers: 700, epoch: 2 | loss: 0.1803370\n",
      "\tspeed: 0.0246s/iter; left time: 71.2294s\n",
      "Epoch: 2 cost time: 17.34662103652954\n",
      "Epoch: 2, Steps: 720 | Train Loss: 0.2114241 Vali Loss: 0.3193977 Test Loss: 0.2426215\n",
      "Validation loss decreased (0.335708 --> 0.319398).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1615143\n",
      "\tspeed: 0.0574s/iter; left time: 159.7328s\n",
      "\titers: 200, epoch: 3 | loss: 0.1860281\n",
      "\tspeed: 0.0232s/iter; left time: 62.2294s\n",
      "\titers: 300, epoch: 3 | loss: 0.2088858\n",
      "\tspeed: 0.0229s/iter; left time: 59.2245s\n",
      "\titers: 400, epoch: 3 | loss: 0.2383755\n",
      "\tspeed: 0.0219s/iter; left time: 54.3758s\n",
      "\titers: 500, epoch: 3 | loss: 0.1827029\n",
      "\tspeed: 0.0238s/iter; left time: 56.7310s\n",
      "\titers: 600, epoch: 3 | loss: 0.1046659\n",
      "\tspeed: 0.0259s/iter; left time: 59.1319s\n",
      "\titers: 700, epoch: 3 | loss: 0.1478776\n",
      "\tspeed: 0.0240s/iter; left time: 52.2375s\n",
      "Epoch: 3 cost time: 17.09708523750305\n",
      "Epoch: 3, Steps: 720 | Train Loss: 0.1869134 Vali Loss: 0.3341665 Test Loss: 0.2517546\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1764761\n",
      "\tspeed: 0.0549s/iter; left time: 113.1747s\n",
      "\titers: 200, epoch: 4 | loss: 0.1723886\n",
      "\tspeed: 0.0243s/iter; left time: 47.7031s\n",
      "\titers: 300, epoch: 4 | loss: 0.1663080\n",
      "\tspeed: 0.0244s/iter; left time: 45.3418s\n",
      "\titers: 400, epoch: 4 | loss: 0.2015989\n",
      "\tspeed: 0.0257s/iter; left time: 45.3188s\n",
      "\titers: 500, epoch: 4 | loss: 0.1746722\n",
      "\tspeed: 0.0243s/iter; left time: 40.4448s\n",
      "\titers: 600, epoch: 4 | loss: 0.1528782\n",
      "\tspeed: 0.0245s/iter; left time: 38.2278s\n",
      "\titers: 700, epoch: 4 | loss: 0.2685850\n",
      "\tspeed: 0.0235s/iter; left time: 34.3407s\n",
      "Epoch: 4 cost time: 17.419237852096558\n",
      "Epoch: 4, Steps: 720 | Train Loss: 0.1731388 Vali Loss: 0.3164384 Test Loss: 0.2463068\n",
      "Validation loss decreased (0.319398 --> 0.316438).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1933678\n",
      "\tspeed: 0.0581s/iter; left time: 77.9538s\n",
      "\titers: 200, epoch: 5 | loss: 0.2202044\n",
      "\tspeed: 0.0244s/iter; left time: 30.3296s\n",
      "\titers: 300, epoch: 5 | loss: 0.1899963\n",
      "\tspeed: 0.0236s/iter; left time: 26.9688s\n",
      "\titers: 400, epoch: 5 | loss: 0.1887054\n",
      "\tspeed: 0.0230s/iter; left time: 23.9176s\n",
      "\titers: 500, epoch: 5 | loss: 0.1309830\n",
      "\tspeed: 0.0224s/iter; left time: 21.0827s\n",
      "\titers: 600, epoch: 5 | loss: 0.1923587\n",
      "\tspeed: 0.0237s/iter; left time: 19.9097s\n",
      "\titers: 700, epoch: 5 | loss: 0.1157891\n",
      "\tspeed: 0.0240s/iter; left time: 17.7658s\n",
      "Epoch: 5 cost time: 17.192835807800293\n",
      "Epoch: 5, Steps: 720 | Train Loss: 0.1644248 Vali Loss: 0.3164015 Test Loss: 0.2372816\n",
      "Validation loss decreased (0.316438 --> 0.316402).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1969944\n",
      "\tspeed: 0.0568s/iter; left time: 35.2691s\n",
      "\titers: 200, epoch: 6 | loss: 0.0969889\n",
      "\tspeed: 0.0240s/iter; left time: 12.4906s\n",
      "\titers: 300, epoch: 6 | loss: 0.1017766\n",
      "\tspeed: 0.0230s/iter; left time: 9.6773s\n",
      "\titers: 400, epoch: 6 | loss: 0.1972107\n",
      "\tspeed: 0.0231s/iter; left time: 7.4176s\n",
      "\titers: 500, epoch: 6 | loss: 0.2429617\n",
      "\tspeed: 0.0255s/iter; left time: 5.6391s\n",
      "\titers: 600, epoch: 6 | loss: 0.1755779\n",
      "\tspeed: 0.0246s/iter; left time: 2.9799s\n",
      "\titers: 700, epoch: 6 | loss: 0.1257797\n",
      "\tspeed: 0.0244s/iter; left time: 0.5117s\n",
      "Epoch: 6 cost time: 17.163996934890747\n",
      "Epoch: 6, Steps: 720 | Train Loss: 0.1592898 Vali Loss: 0.3155497 Test Loss: 0.2357111\n",
      "Validation loss decreased (0.316402 --> 0.315550).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      ">>>>>>>testing : informer_traffic_full_ftS_sl24_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6357\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/main_informer.py\", line 113, in <module>\n",
      "    test_time = exp.test(setting)\n",
      "                ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/exp/exp_informer.py\", line 213, in test\n",
      "    pred, true = self._process_one_batch(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/exp/exp_informer.py\", line 294, in _process_one_batch\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/model.py\", line 73, in forward\n",
      "    dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/decoder.py\", line 46, in forward\n",
      "    x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/decoder.py\", line 21, in forward\n",
      "    x = x + self.dropout(self.self_attention(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/attn.py\", line 153, in forward\n",
      "    out, attn = self.inner_attention(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/attn.py\", line 123, in forward\n",
      "    context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/attn.py\", line 85, in _update_context\n",
      "    attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/utils/masking.py\", line 19, in __init__\n",
      "    index, :].to(device)\n",
      "              ^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Running seq_len=48\n",
      "Args in experiment:\n",
      "Namespace(model='informer', data='traffic_full', root_path='../', data_path='traffic_full.csv', features='S', target='flow', freq='h', checkpoints='./checkpoints/', seq_len=48, label_len=24, pred_len=6, enc_in=1, dec_in=1, c_out=1, d_model=512, n_heads=8, e_layers=2, d_layers=1, s_layers=[3, 2, 1], d_ff=2048, factor=5, padding=0, distil=True, dropout=0.05, attn='prob', embed='timeF', activation='gelu', output_attention=False, do_predict=False, mix=True, cols=None, num_workers=0, itr=10, train_epochs=6, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, inverse=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', detail_freq='h')\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_traffic_full_ftS_sl48_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 24491\n",
      "[DEBUG] Original dataset length: 24491\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 22641\n",
      "val 3503\n",
      "[DEBUG] Original dataset length: 3503\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 3447\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6170\n",
      "\titers: 100, epoch: 1 | loss: 0.2929465\n",
      "\tspeed: 0.0417s/iter; left time: 172.7981s\n",
      "\titers: 200, epoch: 1 | loss: 0.3123372\n",
      "\tspeed: 0.0245s/iter; left time: 98.8979s\n",
      "\titers: 300, epoch: 1 | loss: 0.2047415\n",
      "\tspeed: 0.0243s/iter; left time: 95.9855s\n",
      "\titers: 400, epoch: 1 | loss: 0.2654917\n",
      "\tspeed: 0.0240s/iter; left time: 92.3531s\n",
      "\titers: 500, epoch: 1 | loss: 0.1576056\n",
      "\tspeed: 0.0245s/iter; left time: 91.6298s\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/main_informer.py\", line 109, in <module>\n",
      "    saved_model, train_time = exp.train(setting)\n",
      "                              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/exp/exp_informer.py\", line 159, in train\n",
      "    pred, true = self._process_one_batch(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/exp/exp_informer.py\", line 294, in _process_one_batch\n",
      "    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/model.py\", line 73, in forward\n",
      "    dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/decoder.py\", line 46, in forward\n",
      "    x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/code/transformer/Informer2020/models/decoder.py\", line 34, in forward\n",
      "    y = self.dropout(self.conv2(y).transpose(-1,1))\n",
      "                     ^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 371, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yl2672496l/Yue/anaconda3/envs/transformer/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 366, in _conv_forward\n",
      "    return F.conv1d(\n",
      "           ^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Running seq_len=72\n",
      "Args in experiment:\n",
      "Namespace(model='informer', data='traffic_full', root_path='../', data_path='traffic_full.csv', features='S', target='flow', freq='h', checkpoints='./checkpoints/', seq_len=72, label_len=24, pred_len=6, enc_in=1, dec_in=1, c_out=1, d_model=512, n_heads=8, e_layers=2, d_layers=1, s_layers=[3, 2, 1], d_ff=2048, factor=5, padding=0, distil=True, dropout=0.05, attn='prob', embed='timeF', activation='gelu', output_attention=False, do_predict=False, mix=True, cols=None, num_workers=0, itr=10, train_epochs=6, batch_size=32, patience=3, learning_rate=0.0001, des='Exp', loss='mse', lradj='type1', use_amp=False, inverse=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', detail_freq='h')\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_traffic_full_ftS_sl72_ll24_pl6_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 24467\n",
      "[DEBUG] Original dataset length: 24467\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 22233\n",
      "val 3503\n",
      "[DEBUG] Original dataset length: 3503\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 3423\n",
      "test 7007\n",
      "[DEBUG] Original dataset length: 7007\n",
      "[DEBUG] Cleaned dataset length (NaN removed): 6002\n",
      "\titers: 100, epoch: 1 | loss: 0.2796102\n",
      "\tspeed: 0.0429s/iter; left time: 174.4678s\n",
      "\titers: 200, epoch: 1 | loss: 0.2305377\n",
      "\tspeed: 0.0255s/iter; left time: 101.0079s\n",
      "\titers: 300, epoch: 1 | loss: 0.1474606\n",
      "\tspeed: 0.0255s/iter; left time: 98.3907s\n",
      "\titers: 400, epoch: 1 | loss: 0.3391453\n",
      "\tspeed: 0.0256s/iter; left time: 96.3054s\n",
      "\titers: 500, epoch: 1 | loss: 0.2411745\n",
      "\tspeed: 0.0267s/iter; left time: 97.7560s\n",
      "\titers: 600, epoch: 1 | loss: 0.1840287\n",
      "\tspeed: 0.0273s/iter; left time: 97.1618s\n",
      "Epoch: 1 cost time: 18.801316738128662\n",
      "Epoch: 1, Steps: 694 | Train Loss: 0.2794763 Vali Loss: 0.3488897 Test Loss: 0.2913132\n",
      "Validation loss decreased (inf --> 0.348890).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2181343\n",
      "\tspeed: 0.0806s/iter; left time: 271.7094s\n",
      "\titers: 200, epoch: 2 | loss: 0.2038225\n",
      "\tspeed: 0.0267s/iter; left time: 87.2778s\n"
     ]
    }
   ],
   "source": [
    "for seq_len in seq_lens:\n",
    "    print(f\"Running seq_len={seq_len}\")\n",
    "    !python -u main_informer.py --model informer --data traffic_full --root_path ../ --features S --seq_len {seq_len} --label_len 24 --pred_len 6 --e_layers 2 --d_layers 1 --attn prob --des 'Exp' --itr 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6691ac-0a81-4ead-a255-8904e5bafeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.32990809,  1.68137169])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"./results/running_time_24.npy\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9a808-f64c-43f8-b7da-2d46bae22c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
