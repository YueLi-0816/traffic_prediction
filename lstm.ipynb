{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tKUGmv5cn6GV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8uafyjo7odXA"
   },
   "outputs": [],
   "source": [
    "traffic_data = pd.read_csv('../Traffic data/SCOOT/data_470_hourly/flows/GD030A_S.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZRSO8dF_Sik"
   },
   "source": [
    "## 1. Recover timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4TWZuMkMiYtJ"
   },
   "outputs": [],
   "source": [
    "# Define the recover_timestamp function\n",
    "def recover_timestamp(data):\n",
    "    # Combine 'date' and 'time' to form a datetime column\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'].astype(str) + ':00', format='%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Set 'datetime' as index\n",
    "    data = data.set_index('datetime')\n",
    "\n",
    "    # Create a complete range of timestamps with hourly frequency\n",
    "    full_time_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='H')\n",
    "\n",
    "    # Reindex the data to include all timestamps, filling missing rows with NaN\n",
    "    data_full = data.reindex(full_time_range)\n",
    "\n",
    "    return data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rwy81CAA_Y4r",
    "outputId": "9d3c3f5e-37fd-40f5-e31f-060efe8ee18d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-01 00:00:00</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 01:00:00</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 02:00:00</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 03:00:00</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 04:00:00</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30 19:00:00</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>19.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30 20:00:00</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>20.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30 21:00:00</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>21.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30 22:00:00</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30 23:00:00</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>23.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  time   flow\n",
       "2019-10-01 00:00:00  2019-10-01   0.0   15.0\n",
       "2019-10-01 01:00:00  2019-10-01   1.0    9.0\n",
       "2019-10-01 02:00:00  2019-10-01   2.0    9.0\n",
       "2019-10-01 03:00:00  2019-10-01   3.0    7.0\n",
       "2019-10-01 04:00:00  2019-10-01   4.0    9.0\n",
       "...                         ...   ...    ...\n",
       "2023-09-30 19:00:00  2023-09-30  19.0  129.0\n",
       "2023-09-30 20:00:00  2023-09-30  20.0  119.0\n",
       "2023-09-30 21:00:00  2023-09-30  21.0  106.0\n",
       "2023-09-30 22:00:00  2023-09-30  22.0   88.0\n",
       "2023-09-30 23:00:00  2023-09-30  23.0   88.0\n",
       "\n",
       "[35064 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the recover_timestamp function to recover the full time series\n",
    "traffic_full = recover_timestamp(traffic_data)\n",
    "traffic_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z-mdqKewzjt"
   },
   "source": [
    "## 2. Train, validate, test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h60qTUKvwzCb",
    "outputId": "682cc0ab-b2ed-48a1-df5d-6168064efd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of train_set : 0.6227\n",
      "Proportion of valid_set : 0.1876\n",
      "Proportion of test_set : 0.1897\n"
     ]
    }
   ],
   "source": [
    "# train_set = traffic_full[:'2022-02-28 23:00:00']\n",
    "# valid_set = traffic_full['2022-03-01 00:00:00':'2022-12-31 23:00:00']\n",
    "# test_set = traffic_full['2023-01-01 00:00:00':]\n",
    "train_set = traffic_full['2022-06-03 00:00:00':'2023-03-31 23:00:00']\n",
    "valid_set = traffic_full['2023-04-01 00:00:00':'2023-06-30 23:00:00']\n",
    "test_set = traffic_full['2023-07-01 00:00:00':]\n",
    "print('Proportion of train_set : {:.4f}'.format(len(train_set)/len(traffic_full['2022-06-03 00:00:00':])))\n",
    "print('Proportion of valid_set : {:.4f}'.format(len(valid_set)/len(traffic_full['2022-06-03 00:00:00':])))\n",
    "print('Proportion of test_set : {:.4f}'.format(len(test_set)/len(traffic_full['2022-06-03 00:00:00':])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-03 00:00:00</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03 01:00:00</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03 02:00:00</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03 03:00:00</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03 04:00:00</th>\n",
       "      <td>2022-06-03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31 19:00:00</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>19.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31 20:00:00</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>20.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31 21:00:00</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>21.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31 22:00:00</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>22.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-31 23:00:00</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>23.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  time   flow\n",
       "2022-06-03 00:00:00  2022-06-03   0.0  102.0\n",
       "2022-06-03 01:00:00  2022-06-03   1.0  127.0\n",
       "2022-06-03 02:00:00  2022-06-03   2.0   60.0\n",
       "2022-06-03 03:00:00  2022-06-03   3.0   12.0\n",
       "2022-06-03 04:00:00  2022-06-03   4.0    9.0\n",
       "...                         ...   ...    ...\n",
       "2023-03-31 19:00:00  2023-03-31  19.0  119.0\n",
       "2023-03-31 20:00:00  2023-03-31  20.0  113.0\n",
       "2023-03-31 21:00:00  2023-03-31  21.0  104.0\n",
       "2023-03-31 22:00:00  2023-03-31  22.0   83.0\n",
       "2023-03-31 23:00:00  2023-03-31  23.0   86.0\n",
       "\n",
       "[7248 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd0hRLpW9gJs"
   },
   "source": [
    "## 3. Normalise the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\AppData\\Local\\Temp\\ipykernel_6540\\1100395100.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set.loc[:, 'flow_scaled'] = scaler.transform(train_set[['flow']])\n",
      "C:\\Users\\Amanda\\AppData\\Local\\Temp\\ipykernel_6540\\1100395100.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_set.loc[:, 'flow_scaled'] = scaler.transform(valid_set[['flow']])\n",
      "C:\\Users\\Amanda\\AppData\\Local\\Temp\\ipykernel_6540\\1100395100.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_set.loc[:, 'flow_scaled'] = scaler.transform(test_set[['flow']])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data's 'flow' feature\n",
    "scaler.fit(train_set[['flow']])\n",
    "\n",
    "# Transform the 'flow' feature in all datasets\n",
    "train_set.loc[:, 'flow_scaled'] = scaler.transform(train_set[['flow']])\n",
    "valid_set.loc[:, 'flow_scaled'] = scaler.transform(valid_set[['flow']])\n",
    "test_set.loc[:, 'flow_scaled'] = scaler.transform(test_set[['flow']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xak17ms1uZ-6"
   },
   "source": [
    "## 4. Split the data into X and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GNVVr-27uWm8"
   },
   "outputs": [],
   "source": [
    "# Define the create_multi_step_sequence function\n",
    "def create_multi_step_sequence(data, last_n_steps, day_lag, week_lag, n_future_steps):\n",
    "    \"\"\"\n",
    "    Create input sequences from data using multiple time windows and multiple future steps as output.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The time series data (1D array or list with possible NaN values)\n",
    "    - last_n_steps: Number of most recent steps to use as part of the input (default 12)\n",
    "    - day_lag: Time lag for the last day's value (24 hours ago, default 24)\n",
    "    - week_lag: Time lag for the last week's value (168 hours ago, default 168)\n",
    "    - n_future_steps: Number of future steps to predict (default 6)\n",
    "\n",
    "    Returns:\n",
    "    - X: Input features combining last_n_steps, last day's value, and last week's value\n",
    "    - y: Output labels (shape: [samples, n_future_steps])\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Loop over the data to create the input-output pairs\n",
    "    for i in range(max(last_n_steps, day_lag, week_lag), len(data) - n_future_steps):\n",
    "        # Input sequence of the last `last_n_steps` observations\n",
    "        input_seq = data['flow'].values[i - last_n_steps:i]\n",
    "\n",
    "        # Last day's observation (24 hours ago)\n",
    "        last_day_value = data['flow'].values[i - day_lag]\n",
    "\n",
    "        # Last week's observation (168 hours ago)\n",
    "        last_week_value = data['flow'].values[i - week_lag]\n",
    "\n",
    "        # Output (next `n_future_steps` observations)\n",
    "        output_seq = data['flow'].values[i:i + n_future_steps]\n",
    "\n",
    "        # Check if any NaN values exist in the input or output sequences\n",
    "        if not np.isnan(input_seq).any() and not np.isnan(last_day_value) and not np.isnan(last_week_value) and not np.isnan(output_seq).any():\n",
    "            # Combine the features: last_n_steps, last_day_value, last_week_value\n",
    "            X.append(np.concatenate([input_seq, [last_day_value], [last_week_value]]))\n",
    "            y.append(output_seq)\n",
    "\n",
    "    # Convert to numpy arrays and reshape X to match CNN expected input (samples, timesteps, features)\n",
    "    X = np.array(X).reshape(-1, last_n_steps + 2, 1)  # Add the 2 additional features (last_day_value, last_week_value)\n",
    "    y = np.array(y).reshape(-1, n_future_steps)  # Multiple output steps\n",
    "\n",
    "    # Convert the entire input and output into a pandas DataFrame with appropriate column names for multi-step prediction\n",
    "\n",
    "    # Define column names for the input DataFrame\n",
    "    input_columns = [f'Step_{i}_back' for i in range(last_n_steps, 0, -1)] + ['Day_1_back', 'Week_1_back']\n",
    "\n",
    "    # Create a DataFrame for the input (all rows)\n",
    "    df_X = pd.DataFrame(X.reshape(X.shape[0], 14), columns=input_columns)\n",
    "\n",
    "    # Create a DataFrame for the output (all rows), where each column represents one of the next 6 steps\n",
    "    output_columns = [f'Next_Step_{i}' for i in range(0, n_future_steps)]\n",
    "    df_y = pd.DataFrame(y, columns=output_columns)\n",
    "\n",
    "    return X, y, df_X, df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to predict next 6 steps simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use\n",
    "* the last 12 steps\n",
    "\n",
    "* previous one week (24 steps)\n",
    "\n",
    "* previous one month  (168 steps)\n",
    "\n",
    "*  to forecast current (0 step) and 5 steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1141040 into shape (168,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create input-output sequences with the provided function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, y_train, X_train_df, y_train_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_multi_step_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_lag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweek_lag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_future_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_valid, y_valid, X_valid_df, y_valid_df \u001b[38;5;241m=\u001b[39m create_multi_step_sequence(valid_set, last_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m7\u001b[39m, day_lag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, week_lag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_future_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_test, y_test, X_test_df, y_test_df \u001b[38;5;241m=\u001b[39m create_multi_step_sequence(test_set, last_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m7\u001b[39m, day_lag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, week_lag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_future_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mcreate_multi_step_sequence\u001b[1;34m(data, last_n_steps, day_lag, week_lag, n_future_steps)\u001b[0m\n\u001b[0;32m     37\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(output_seq)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Convert to numpy arrays and reshape X to match CNN expected input (samples, timesteps, features)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_n_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add the 2 additional features (last_day_value, last_week_value)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_future_steps)  \u001b[38;5;66;03m# Multiple output steps\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Convert the entire input and output into a pandas DataFrame with appropriate column names for multi-step prediction\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Define column names for the input DataFrame\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1141040 into shape (168,1)"
     ]
    }
   ],
   "source": [
    "# Create input-output sequences with the provided function\n",
    "X_train, y_train, X_train_df, y_train_df = create_multi_step_sequence(train_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=6)\n",
    "X_valid, y_valid, X_valid_df, y_valid_df = create_multi_step_sequence(valid_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=6)\n",
    "X_test, y_test, X_test_df, y_test_df = create_multi_step_sequence(test_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model_multi_step(input_shape, n_outputs, units, dropout_rate, learning_rate):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, activation='tanh', input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_outputs))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss='mse',  # Mean Squared Error loss for regression\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mae']  # Mean Absolute Error as a metric\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape = (x_train_scaled.shape[1], x_train_scaled.shape[2])\n",
    "n_outputs = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCIZFe1MAx-X"
   },
   "source": [
    "## 6. Set Up Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter options\n",
    "units_list = [50, 100, 200]\n",
    "dropout_rates = [0, 0.3, 0.5]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "# Create all possible combinations\n",
    "hyperparameter_combinations = list(product(units_list, dropout_rates, learning_rates, batch_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ixRU8sQA8NW"
   },
   "source": [
    "## 7. Train the Model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\program\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Validation MAE: 0.0602\n",
      "\n",
      "Combination 2/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Validation MAE: 0.0600\n",
      "\n",
      "Combination 3/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Validation MAE: 0.0600\n",
      "\n",
      "Combination 4/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0618\n",
      "\n",
      "Combination 5/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Validation MAE: 0.0634\n",
      "\n",
      "Combination 6/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0644\n",
      "\n",
      "Combination 7/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Epoch 47: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Validation MAE: 0.0685\n",
      "\n",
      "Combination 8/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0685\n",
      "\n",
      "Combination 9/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0700\n",
      "\n",
      "Combination 10/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Validation MAE: 0.0610\n",
      "\n",
      "Combination 11/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Validation MAE: 0.0604\n",
      "\n",
      "Combination 12/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Validation MAE: 0.0611\n",
      "\n",
      "Combination 13/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0619\n",
      "\n",
      "Combination 14/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0640\n",
      "\n",
      "Combination 15/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0652\n",
      "\n",
      "Combination 16/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0688\n",
      "\n",
      "Combination 17/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0711\n",
      "\n",
      "Combination 18/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0740\n",
      "\n",
      "Combination 19/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0610\n",
      "\n",
      "Combination 20/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Validation MAE: 0.0614\n",
      "\n",
      "Combination 21/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0603\n",
      "\n",
      "Combination 22/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0630\n",
      "\n",
      "Combination 23/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Validation MAE: 0.0658\n",
      "\n",
      "Combination 24/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0657\n",
      "\n",
      "Combination 25/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0686\n",
      "\n",
      "Combination 26/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0720\n",
      "\n",
      "Combination 27/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0745\n",
      "\n",
      "Combination 28/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=32\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Validation MAE: 0.0608\n",
      "\n",
      "Combination 29/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Validation MAE: 0.0608\n",
      "\n",
      "Combination 30/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 36: early stopping\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Validation MAE: 0.0606\n",
      "\n",
      "Combination 31/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Validation MAE: 0.0603\n",
      "\n",
      "Combination 32/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0616\n",
      "\n",
      "Combination 33/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Validation MAE: 0.0633\n",
      "\n",
      "Combination 34/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0669\n",
      "\n",
      "Combination 35/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0679\n",
      "\n",
      "Combination 36/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0682\n",
      "\n",
      "Combination 37/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Validation MAE: 0.0606\n",
      "\n",
      "Combination 38/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Validation MAE: 0.0607\n",
      "\n",
      "Combination 39/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Epoch 50: early stopping\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Validation MAE: 0.0595\n",
      "\n",
      "Combination 40/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Validation MAE: 0.0610\n",
      "\n",
      "Combination 41/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0618\n",
      "\n",
      "Combination 42/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0643\n",
      "\n",
      "Combination 43/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0675\n",
      "\n",
      "Combination 44/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0687\n",
      "\n",
      "Combination 45/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0708\n",
      "\n",
      "Combination 46/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0605\n",
      "\n",
      "Combination 47/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Validation MAE: 0.0606\n",
      "\n",
      "Combination 48/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Validation MAE: 0.0608\n",
      "\n",
      "Combination 49/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0616\n",
      "\n",
      "Combination 50/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0632\n",
      "\n",
      "Combination 51/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Validation MAE: 0.0654\n",
      "\n",
      "Combination 52/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0672\n",
      "\n",
      "Combination 53/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0690\n",
      "\n",
      "Combination 54/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0712\n",
      "\n",
      "Combination 55/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=32\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0600\n",
      "\n",
      "Combination 56/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "Validation MAE: 0.0604\n",
      "\n",
      "Combination 57/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0605\n",
      "\n",
      "Combination 58/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Epoch 47: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "Validation MAE: 0.0605\n",
      "\n",
      "Combination 59/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0601\n",
      "\n",
      "Combination 60/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0618\n",
      "\n",
      "Combination 61/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0658\n",
      "\n",
      "Combination 62/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0672\n",
      "\n",
      "Combination 63/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Validation MAE: 0.0688\n",
      "\n",
      "Combination 64/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Validation MAE: 0.0612\n",
      "\n",
      "Combination 65/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Validation MAE: 0.0612\n",
      "\n",
      "Combination 66/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0607\n",
      "\n",
      "Combination 67/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0603\n",
      "\n",
      "Combination 68/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0607\n",
      "\n",
      "Combination 69/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0631\n",
      "\n",
      "Combination 70/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0659\n",
      "\n",
      "Combination 71/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0682\n",
      "\n",
      "Combination 72/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0688\n",
      "\n",
      "Combination 73/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Validation MAE: 0.0596\n",
      "\n",
      "Combination 74/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 36: early stopping\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Validation MAE: 0.0595\n",
      "\n",
      "Combination 75/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Validation MAE: 0.0605\n",
      "\n",
      "Combination 76/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0607\n",
      "\n",
      "Combination 77/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0612\n",
      "\n",
      "Combination 78/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0635\n",
      "\n",
      "Combination 79/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0661\n",
      "\n",
      "Combination 80/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0679\n",
      "\n",
      "Combination 81/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0689\n",
      "\n",
      "Best Hyperparameters:\n",
      "units: 100\n",
      "dropout_rate: 0.3\n",
      "learning_rate: 0.01\n",
      "batch_size: 128\n",
      "Best Validation MAE: 0.0595\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best model and hyperparameters\n",
    "best_val_mae = np.inf\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "for idx, (units, dropout_rate, learning_rate, batch_size) in enumerate(hyperparameter_combinations):\n",
    "    print(f\"\\nCombination {idx+1}/{len(hyperparameter_combinations)}\")\n",
    "    print(f\"Training with units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "    # Create the LSTM model with the current hyperparameters\n",
    "    model = create_lstm_model_multi_step(\n",
    "        input_shape=input_shape,\n",
    "        n_outputs=n_outputs,\n",
    "        units=units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train_scaled, y_train_scaled,\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_val_scaled, y_val_scaled),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Get the best validation MAE from this training run\n",
    "    val_mae = min(history.history['val_mae'])\n",
    "    print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Update best model if current one is better\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_model = model\n",
    "        best_hyperparams = {\n",
    "            'units': units,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_hyperparams.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Best Validation MAE: {best_val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13:33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters for full time:\n",
    "units: 200\n",
    "dropout_rate: 0.5\n",
    "learning_rate: 0.01\n",
    "batch_size: 32\n",
    "Best Validation MAE: 0.0503\n",
    "\n",
    "Best Hyperparameters for after covid:\n",
    "units: 100\n",
    "dropout_rate: 0.3\n",
    "learning_rate: 0.01\n",
    "batch_size: 128\n",
    "Best Validation MAE: 0.0595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_scaled = best_model.predict(x_test_scaled)\n",
    "\n",
    "# Reshape for inverse scaling\n",
    "y_pred_reshaped = y_pred_scaled.reshape(-1, 1)\n",
    "y_test_reshaped = y_test_scaled.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform\n",
    "y_pred_inverse = y_scaler.inverse_transform(y_pred_reshaped).reshape(n_test_samples, n_outputs)\n",
    "y_test_inverse = y_scaler.inverse_transform(y_test_reshaped).reshape(n_test_samples, n_outputs)\n",
    "\n",
    "y_test_flat = y_test_inverse.flatten()\n",
    "y_pred_flat = y_pred_inverse.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SREwnRqxMN1"
   },
   "source": [
    "## 9. Evaluating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time Step 1 Evaluation Metrics:\n",
      "RMSE: 28.4454\n",
      "MAE: 20.8450\n",
      "MAPE: 19.44%\n",
      "\n",
      "Time Step 2 Evaluation Metrics:\n",
      "RMSE: 36.2563\n",
      "MAE: 26.7673\n",
      "MAPE: 25.02%\n",
      "\n",
      "Time Step 3 Evaluation Metrics:\n",
      "RMSE: 43.7203\n",
      "MAE: 33.1341\n",
      "MAPE: 29.97%\n",
      "\n",
      "Time Step 4 Evaluation Metrics:\n",
      "RMSE: 49.3573\n",
      "MAE: 38.0972\n",
      "MAPE: 35.19%\n",
      "\n",
      "Time Step 5 Evaluation Metrics:\n",
      "RMSE: 53.2812\n",
      "MAE: 41.5888\n",
      "MAPE: 39.42%\n",
      "\n",
      "Time Step 6 Evaluation Metrics:\n",
      "RMSE: 53.3926\n",
      "MAE: 42.0135\n",
      "MAPE: 44.19%\n"
     ]
    }
   ],
   "source": [
    "# Compute Metrics for Each Time Step\n",
    "\n",
    "for i in range(n_outputs):\n",
    "    y_true = y_test_inverse[:, i]\n",
    "    y_pred = y_pred_inverse[:, i]\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    # Avoid division by zero by adding a small epsilon to y_test_flat if necessary\n",
    "    epsilon = 1e-10\n",
    "    y_true_safe = np.where(y_true == 0, epsilon, y_true)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
    "\n",
    "    print(f\"\\nTime Step {i+1} Evaluation Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyYn2XL2GUxw"
   },
   "source": [
    "# Code to predict next 6 steps step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vavf9D4-lc6E"
   },
   "source": [
    "#### We will use\n",
    "* the last 12 steps\n",
    "\n",
    "* previous one week (24 steps)\n",
    "\n",
    "* previous one month  (168 steps)\n",
    "\n",
    "*  to forecast current (0 step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfV8VwesmXS3"
   },
   "source": [
    "## 4. Create input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PJaM65dTlc6E"
   },
   "outputs": [],
   "source": [
    "# Create input-output sequences with the provided function\n",
    "X_train, y_train, X_train_df, y_train_df = create_multi_step_sequence(train_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=1)\n",
    "X_valid, y_valid, X_valid_df, y_valid_df = create_multi_step_sequence(valid_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=1)\n",
    "X_test, y_test, X_test_df, y_test_df = create_multi_step_sequence(test_set, last_n_steps=12, day_lag=24, week_lag=168, n_future_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipA2bPAcm2L1",
    "outputId": "687db2e3-3f26-4cc3-c15b-0d8066617ff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19570, 14, 1), (6907, 14, 1), (5623, 14, 1))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GNpslqvlx5_"
   },
   "source": [
    "## 5. Normalise the data after split (step-by-step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_Co01Aflx6B"
   },
   "source": [
    "Normalise X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Z7YuPxvRlx6C"
   },
   "outputs": [],
   "source": [
    "# Separate scalers for inputs and outputs\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Reshape x_train to 2D for scaling\n",
    "n_samples, n_timesteps, n_features = X_train.shape\n",
    "x_train_reshaped = X_train.reshape(-1, n_features)  # Shape: (n_samples * n_timesteps, n_features)\n",
    "# Fit the scaler on the training data\n",
    "x_scaler.fit(x_train_reshaped)\n",
    "# Transform the training data\n",
    "x_train_scaled = x_scaler.transform(x_train_reshaped)\n",
    "# Reshape back to original shape\n",
    "x_train_scaled = x_train_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# x_val\n",
    "n_val_samples = X_valid.shape[0]\n",
    "x_val_reshaped = X_valid.reshape(-1, n_features)\n",
    "x_val_scaled = x_scaler.transform(x_val_reshaped)\n",
    "x_val_scaled = x_val_scaled.reshape(n_val_samples, n_timesteps, n_features)\n",
    "\n",
    "# x_test\n",
    "n_test_samples = X_test.shape[0]\n",
    "x_test_reshaped = X_test.reshape(-1, n_features)\n",
    "x_test_scaled = x_scaler.transform(x_test_reshaped)\n",
    "x_test_scaled = x_test_scaled.reshape(n_test_samples, n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw7rcIWPlx6D"
   },
   "source": [
    "Normalise y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "azSWFgFmlx6D"
   },
   "outputs": [],
   "source": [
    "# Reshape y_train to 2D for scaling\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Shape: (n_samples * n_outputs, 1)\n",
    "# Fit the scaler on the training data\n",
    "y_scaler.fit(y_train_reshaped)\n",
    "# Transform the training data\n",
    "y_train_scaled = y_scaler.transform(y_train_reshaped)\n",
    "# Reshape back to original shape\n",
    "y_train_scaled = y_train_scaled.reshape(n_samples, y_train.shape[1])\n",
    "\n",
    "# y_val\n",
    "y_val_reshaped = y_valid.reshape(-1, 1)\n",
    "y_val_scaled = y_scaler.transform(y_val_reshaped)\n",
    "y_val_scaled = y_val_scaled.reshape(n_val_samples, y_valid.shape[1])\n",
    "\n",
    "# y_test\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "y_test_scaled = y_scaler.transform(y_test_reshaped)\n",
    "y_test_scaled = y_test_scaled.reshape(n_test_samples, y_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ7sNErrpc9O"
   },
   "source": [
    "## 6. Build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eE7DAUCIoVSt"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, units, dropout_rate, learning_rate):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, activation='tanh', input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1)) # Output layer for one-step prediction\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape = (x_train_scaled.shape[1], x_train_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYD8kk0NpzNp"
   },
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u4zCXOAo9tN"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter options\n",
    "units_list = [50, 100, 200]\n",
    "dropout_rates = [0, 0.3, 0.5]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "# Create all possible combinations\n",
    "hyperparameter_combinations = list(product(units_list, dropout_rates, learning_rates, batch_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d-_-jwBp45_",
    "outputId": "e551e47c-fc12-4b0c-d237-39b9eb0d4f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\program\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Validation MAE: 0.0452\n",
      "\n",
      "Combination 2/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Validation MAE: 0.0452\n",
      "\n",
      "Combination 3/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Validation MAE: 0.0466\n",
      "\n",
      "Combination 4/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0450\n",
      "\n",
      "Combination 5/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0463\n",
      "\n",
      "Combination 6/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0492\n",
      "\n",
      "Combination 7/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0496\n",
      "\n",
      "Combination 8/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0506\n",
      "\n",
      "Combination 9/81\n",
      "Training with units=50, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0512\n",
      "\n",
      "Combination 10/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0464\n",
      "\n",
      "Combination 11/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Validation MAE: 0.0467\n",
      "\n",
      "Combination 12/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Epoch 50: early stopping\n",
      "Restoring model weights from the end of the best epoch: 40.\n",
      "Validation MAE: 0.0457\n",
      "\n",
      "Combination 13/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0466\n",
      "\n",
      "Combination 14/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Epoch 36: early stopping\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Validation MAE: 0.0498\n",
      "\n",
      "Combination 15/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0493\n",
      "\n",
      "Combination 16/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0508\n",
      "\n",
      "Combination 17/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0516\n",
      "\n",
      "Combination 18/81\n",
      "Training with units=50, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0517\n",
      "\n",
      "Combination 19/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Epoch 41: early stopping\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "Validation MAE: 0.0459\n",
      "\n",
      "Combination 20/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 45: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Validation MAE: 0.0461\n",
      "\n",
      "Combination 21/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Validation MAE: 0.0463\n",
      "\n",
      "Combination 22/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0471\n",
      "\n",
      "Combination 23/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0489\n",
      "\n",
      "Combination 24/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0501\n",
      "\n",
      "Combination 25/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0510\n",
      "\n",
      "Combination 26/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0514\n",
      "\n",
      "Combination 27/81\n",
      "Training with units=50, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0522\n",
      "\n",
      "Combination 28/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=32\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Validation MAE: 0.0463\n",
      "\n",
      "Combination 29/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Validation MAE: 0.0461\n",
      "\n",
      "Combination 30/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Validation MAE: 0.0453\n",
      "\n",
      "Combination 31/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Validation MAE: 0.0450\n",
      "\n",
      "Combination 32/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0458\n",
      "\n",
      "Combination 33/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Validation MAE: 0.0496\n",
      "\n",
      "Combination 34/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0494\n",
      "\n",
      "Combination 35/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0502\n",
      "\n",
      "Combination 36/81\n",
      "Training with units=100, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Validation MAE: 0.0512\n",
      "\n",
      "Combination 37/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Validation MAE: 0.0463\n",
      "\n",
      "Combination 38/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Validation MAE: 0.0459\n",
      "\n",
      "Combination 39/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0457\n",
      "\n",
      "Combination 40/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0456\n",
      "\n",
      "Combination 41/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0469\n",
      "\n",
      "Combination 42/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Validation MAE: 0.0494\n",
      "\n",
      "Combination 43/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Epoch 45: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n",
      "Validation MAE: 0.0502\n",
      "\n",
      "Combination 44/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0509\n",
      "\n",
      "Combination 45/81\n",
      "Training with units=100, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0513\n",
      "\n",
      "Combination 46/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Validation MAE: 0.0466\n",
      "\n",
      "Combination 47/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Validation MAE: 0.0460\n",
      "\n",
      "Combination 48/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "Validation MAE: 0.0458\n",
      "\n",
      "Combination 49/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0463\n",
      "\n",
      "Combination 50/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Validation MAE: 0.0496\n",
      "\n",
      "Combination 51/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Validation MAE: 0.0496\n",
      "\n",
      "Combination 52/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0505\n",
      "\n",
      "Combination 53/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0511\n",
      "\n",
      "Combination 54/81\n",
      "Training with units=100, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Validation MAE: 0.0514\n",
      "\n",
      "Combination 55/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=32\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Validation MAE: 0.0464\n",
      "\n",
      "Combination 56/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=64\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "Validation MAE: 0.0452\n",
      "\n",
      "Combination 57/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.01, batch_size=128\n",
      "Epoch 35: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Validation MAE: 0.0455\n",
      "\n",
      "Combination 58/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0452\n",
      "\n",
      "Combination 59/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0459\n",
      "\n",
      "Combination 60/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Validation MAE: 0.0470\n",
      "\n",
      "Combination 61/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0492\n",
      "\n",
      "Combination 62/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0497\n",
      "\n",
      "Combination 63/81\n",
      "Training with units=200, dropout_rate=0, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0503\n",
      "\n",
      "Combination 64/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Validation MAE: 0.0460\n",
      "\n",
      "Combination 65/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=64\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Validation MAE: 0.0461\n",
      "\n",
      "Combination 66/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.01, batch_size=128\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Validation MAE: 0.0486\n",
      "\n",
      "Combination 67/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0460\n",
      "\n",
      "Combination 68/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0460\n",
      "\n",
      "Combination 69/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0471\n",
      "\n",
      "Combination 70/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0495\n",
      "\n",
      "Combination 71/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "Validation MAE: 0.0499\n",
      "\n",
      "Combination 72/81\n",
      "Training with units=200, dropout_rate=0.3, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0504\n",
      "\n",
      "Combination 73/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=32\n",
      "Epoch 37: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Validation MAE: 0.0455\n",
      "\n",
      "Combination 74/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=64\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Validation MAE: 0.0466\n",
      "\n",
      "Combination 75/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.01, batch_size=128\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Validation MAE: 0.0465\n",
      "\n",
      "Combination 76/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=32\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Validation MAE: 0.0467\n",
      "\n",
      "Combination 77/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=64\n",
      "Epoch 38: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Validation MAE: 0.0478\n",
      "\n",
      "Combination 78/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "Validation MAE: 0.0477\n",
      "\n",
      "Combination 79/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=32\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Validation MAE: 0.0496\n",
      "\n",
      "Combination 80/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=64\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "Validation MAE: 0.0502\n",
      "\n",
      "Combination 81/81\n",
      "Training with units=200, dropout_rate=0.5, learning_rate=0.0001, batch_size=128\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Validation MAE: 0.0508\n",
      "\n",
      "Best Hyperparameters:\n",
      "units: 100\n",
      "dropout_rate: 0\n",
      "learning_rate: 0.001\n",
      "batch_size: 32\n",
      "Best Validation MAE: 0.0450\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best model and hyperparameters\n",
    "best_val_mae = np.inf\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "for idx, (units, dropout_rate, learning_rate, batch_size) in enumerate(hyperparameter_combinations):\n",
    "    print(f\"\\nCombination {idx+1}/{len(hyperparameter_combinations)}\")\n",
    "    print(f\"Training with units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "    # Create the LSTM model with the current hyperparameters\n",
    "    model = create_lstm_model(\n",
    "        input_shape=input_shape,\n",
    "        units=units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train_scaled, y_train_scaled,\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_val_scaled, y_val_scaled),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Get the best validation MAE from this training run\n",
    "    val_mae = min(history.history['val_mae'])\n",
    "    print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Update best model if current one is better\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_model = model\n",
    "        best_hyperparams = {\n",
    "            'units': units,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_hyperparams.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Best Validation MAE: {best_val_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters full time:\n",
    "units: 50\n",
    "dropout_rate: 0\n",
    "learning_rate: 0.01\n",
    "batch_size: 128\n",
    "Best Validation MAE: 0.0388\n",
    "\n",
    "Best Hyperparameters after covid:\n",
    "units: 100\n",
    "dropout_rate: 0\n",
    "learning_rate: 0.001\n",
    "batch_size: 32\n",
    "Best Validation MAE: 0.0450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate the model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lstm_model(input_shape, units, dropout_rate, learning_rate):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(units=units, activation='tanh', input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1)) # Output layer for one-step prediction\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train_scaled.shape[1], x_train_scaled.shape[2])\n",
    "units=50\n",
    "dropout_rate=0\n",
    "learning_rate=0.01\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0044 - mae: 0.0474 - val_loss: 0.0039 - val_mae: 0.0405\n",
      "Epoch 2/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0043 - mae: 0.0465 - val_loss: 0.0039 - val_mae: 0.0406\n",
      "Epoch 3/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0467 - val_loss: 0.0040 - val_mae: 0.0432\n",
      "Epoch 4/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0044 - mae: 0.0472 - val_loss: 0.0039 - val_mae: 0.0401\n",
      "Epoch 5/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0469 - val_loss: 0.0040 - val_mae: 0.0430\n",
      "Epoch 6/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0044 - mae: 0.0473 - val_loss: 0.0039 - val_mae: 0.0399\n",
      "Epoch 7/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0464 - val_loss: 0.0039 - val_mae: 0.0406\n",
      "Epoch 8/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0045 - mae: 0.0475 - val_loss: 0.0039 - val_mae: 0.0416\n",
      "Epoch 9/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 0.0043 - mae: 0.0464 - val_loss: 0.0039 - val_mae: 0.0411\n",
      "Epoch 10/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0044 - mae: 0.0468 - val_loss: 0.0039 - val_mae: 0.0406\n",
      "Epoch 11/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0043 - mae: 0.0463 - val_loss: 0.0039 - val_mae: 0.0404\n",
      "Epoch 12/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0459 - val_loss: 0.0041 - val_mae: 0.0421\n",
      "Epoch 13/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0043 - mae: 0.0464 - val_loss: 0.0039 - val_mae: 0.0400\n",
      "Epoch 14/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0041 - mae: 0.0459 - val_loss: 0.0039 - val_mae: 0.0394\n",
      "Epoch 15/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0043 - mae: 0.0466 - val_loss: 0.0039 - val_mae: 0.0411\n",
      "Epoch 16/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0460 - val_loss: 0.0039 - val_mae: 0.0401\n",
      "Epoch 17/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0042 - mae: 0.0459 - val_loss: 0.0039 - val_mae: 0.0399\n",
      "Epoch 18/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0462 - val_loss: 0.0040 - val_mae: 0.0411\n",
      "Epoch 19/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 0.0041 - mae: 0.0456 - val_loss: 0.0039 - val_mae: 0.0394\n",
      "Epoch 20/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0040 - mae: 0.0454 - val_loss: 0.0039 - val_mae: 0.0405\n",
      "Epoch 21/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0044 - mae: 0.0467 - val_loss: 0.0040 - val_mae: 0.0409\n",
      "Epoch 22/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 0.0042 - mae: 0.0461 - val_loss: 0.0041 - val_mae: 0.0421\n",
      "Epoch 23/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0464 - val_loss: 0.0040 - val_mae: 0.0409\n",
      "Epoch 24/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0042 - mae: 0.0462 - val_loss: 0.0040 - val_mae: 0.0396\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = best_model.fit(\n",
    "    x_train_scaled, y_train_scaled,\n",
    "    epochs=50,  # You can adjust this as needed\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val_scaled, y_val_scaled),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1  # Set to 1 to see detailed training output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27LXIyi-w0th"
   },
   "source": [
    "## 8. Recursive Forecasting with LSTM (step-by-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "O5eKQOi8dLQ-"
   },
   "outputs": [],
   "source": [
    "def recursive_forecast(model, x_test_scaled, start_index, n_steps, x_scaler, y_scaler):\n",
    "    \"\"\"\n",
    "    Perform recursive forecasting using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model\n",
    "    - x_test_scaled: The scaled test input data (shape: n_samples, n_timesteps, n_features)\n",
    "    - start_index: The starting index in the test data\n",
    "    - n_steps: Number of future steps to predict\n",
    "    - x_scaler: Scaler used for input features\n",
    "    - y_scaler: Scaler used for target variable\n",
    "\n",
    "    Returns:\n",
    "    - predictions: List of predicted values (in original scale)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    scaled_predictions = []\n",
    "    current_input = x_test_scaled[start_index].copy()  # Shape: (n_timesteps, n_features)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Reshape to (1, n_timesteps, n_features) for prediction\n",
    "        input_seq = current_input.reshape((1, current_input.shape[0], current_input.shape[1]))\n",
    "\n",
    "        # Predict the next time step (scaled)\n",
    "        yhat_scaled = model.predict(input_seq, verbose=0)  # Shape: (1, 1)\n",
    "\n",
    "        # Inverse transform the prediction to original scale\n",
    "        yhat = y_scaler.inverse_transform(yhat_scaled)[0, 0]\n",
    "        \n",
    "        # Transform the prediction back to input feature scale for lag features\n",
    "        yhat_for_input = x_scaler.transform(yhat.reshape(-1, 1))[0, 0]\n",
    "\n",
    "        # Append predictions\n",
    "        predictions.append(yhat)\n",
    "        scaled_predictions.append(yhat_for_input)\n",
    "\n",
    "        # Move to the next time step in x_test_scaled\n",
    "        next_index = start_index + step + 1\n",
    "        if next_index < len(x_test_scaled):\n",
    "            # Use features from the next time step\n",
    "            next_input = x_test_scaled[next_index].copy()\n",
    "        else:\n",
    "            # Reached the end of x_test_scaled\n",
    "            break\n",
    "\n",
    "        # Update lag features with available scaled predictions\n",
    "        for lag in range(1, min(step + 1, 6) + 1):\n",
    "            feature_index = 12 - lag  # lag1 is at index 11\n",
    "            next_input[feature_index, 0] = scaled_predictions[-lag]\n",
    "\n",
    "        # Keep lag24 and lag168 as they are, or update if necessary\n",
    "\n",
    "        # Set current_input for next iteration\n",
    "        current_input = next_input\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBK2TyUSxCQ-"
   },
   "source": [
    "## 9. Make step-by-step prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "79gLJQDkdNBl"
   },
   "outputs": [],
   "source": [
    "# Number of steps to predict\n",
    "n_steps = 6\n",
    "\n",
    "# Initialize lists to store predictions and actual values\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "# Ensure we have enough data for recursive predictions\n",
    "n_test_samples = x_test_scaled.shape[0]\n",
    "\n",
    "for i in range(n_test_samples - n_steps):\n",
    "    # Perform recursive forecasting\n",
    "    predictions = recursive_forecast(\n",
    "        model=best_model,\n",
    "        x_test_scaled=x_test_scaled,\n",
    "        start_index=i,\n",
    "        n_steps=n_steps,\n",
    "        x_scaler=x_scaler,\n",
    "        y_scaler=y_scaler\n",
    "    )\n",
    "\n",
    "    # Get the actual future values (in original scale)\n",
    "    actual_values = y_test[i+1:i + len(predictions) + 1].flatten()\n",
    "\n",
    "    # Store the predictions and actual values\n",
    "    all_predictions.append(predictions)\n",
    "    all_actuals.append(actual_values)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SREwnRqxMN1"
   },
   "source": [
    "## 10. Evaluating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xw9n5yNExAkJ",
    "outputId": "4f5b2721-8d8e-4871-eb56-dfd1b17ab14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time Step 1 Evaluation Metrics:\n",
      "RMSE: 41.4159\n",
      "MAE: 29.8309\n",
      "MAPE: 32.89%\n",
      "\n",
      "Time Step 2 Evaluation Metrics:\n",
      "RMSE: 44.7596\n",
      "MAE: 31.9444\n",
      "MAPE: 36.16%\n",
      "\n",
      "Time Step 3 Evaluation Metrics:\n",
      "RMSE: 47.2820\n",
      "MAE: 33.4781\n",
      "MAPE: 39.15%\n",
      "\n",
      "Time Step 4 Evaluation Metrics:\n",
      "RMSE: 49.5900\n",
      "MAE: 34.8770\n",
      "MAPE: 41.75%\n",
      "\n",
      "Time Step 5 Evaluation Metrics:\n",
      "RMSE: 51.5312\n",
      "MAE: 35.9326\n",
      "MAPE: 43.32%\n",
      "\n",
      "Time Step 6 Evaluation Metrics:\n",
      "RMSE: 53.2142\n",
      "MAE: 36.6299\n",
      "MAPE: 44.43%\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "epsilon = 1e-10  # To avoid division by zero in MAPE\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_true = all_actuals[:, i]\n",
    "    y_pred = all_predictions[:, i]\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    y_true_safe = np.where(y_true == 0, epsilon, y_true)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
    "\n",
    "    print(f\"\\nTime Step {i+1} Evaluation Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9VMIQUvBZ3q"
   },
   "outputs": [],
   "source": [
    "step-by-step full date"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
